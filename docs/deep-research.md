Serverless Azure Architecture for YouTube Video Summaries
Overview
This solution converts a public YouTube video URL into a downloadable HTML/Markdown document with synchronized images and transcript text, ideal for importing into Notion. It uses serverless Azure components to remain low-cost and scalable, analyzing the video for slide changes and speaker transitions. The output document will contain key screenshots (images) from the video with corresponding transcript paragraphs beneath each image, segmented at slide change or speaker switch points.
Azure Services and Components
•	Azure Video Indexer – AI service that analyzes the video and extracts insights. It generates a JSON index with the full speech transcript (with timestamps and speaker attribution) and visual cues like shots/scenes and keyframes[1]. This enables detecting slide changes (via shot boundaries) and identifying different speakers[2].
•	Azure Blob Storage – Used to temporarily store the video file (downloaded from YouTube), extracted key images, and the final HTML/Markdown output. Blob Storage provides cheap, scalable storage for large media and results.
•	Azure Functions – Serverless compute for custom logic (in code). Functions handle tasks like downloading the YouTube video, processing Video Indexer results, and formatting the final document. They run on a pay-per-use model and can scale out to handle multiple videos concurrently, contributing to a low-cost design[3].
•	Azure Logic Apps – Orchestration service to coordinate the workflow without manual polling. The Logic App (triggered by an HTTP request from the client) calls the Video Indexer API and Azure Functions in sequence, implementing the process flow as a serverless pipeline. (Alternatively, Azure Durable Functions could orchestrate these steps in code. Both approaches leverage Azure’s serverless platform[4].)
Data Flow and Workflow
1.	HTTP Trigger & Video Ingestion: The process starts when a user submits a YouTube URL (for a public video) via an HTTP request. An Azure Logic App (HTTP-triggered) or Azure Function receives this request and initiates the workflow. The first step is to fetch the video content. An Azure Function can be used here to download the YouTube video (e.g. using a YouTube API or a downloader library) and save it as a video file into Azure Blob Storage. Only public YouTube URLs are supported, so the video file can be accessed without authentication. Storing the raw video in Blob allows Azure services to easily access it for processing[5].
2.	Video Indexing (Analysis): Next, the workflow invokes Azure Video Indexer to analyze the video. This can be done by the Logic App using the “Upload Video and Index” API action, providing either the Blob URL of the video or the video file content. Azure Video Indexer will process the video using its built-in AI models and create an index of insights. Notably, it performs speech-to-text transcription with speaker identification, and scene/shot detection for visual changes[1][2]. Each insight is time-stamped. For example, Video Indexer detects when a new shot begins/ends based on visual transitions (such as a slide change in a presentation)[6], and it tags transcript lines with speaker IDs to mark speaker transitions. This indexing runs asynchronously; to keep the architecture serverless, no VM is needed – the managed Video Indexer service handles the heavy AI processing.
3.	Processing Status Monitoring: The workflow must wait for the indexing to complete. The Logic App can enter a loop or use a delay + retry pattern to poll the Video Indexer for status. For instance, it can call “Get Video Index” periodically until the service reports the video is fully indexed (e.g. a state field becomes “Processed”). This polling is done within the Logic App with a timer (or using Azure Durable Function’s built-in waiting mechanism if that is used). Once indexing is finished, the full insights JSON is retrieved via the Get Video Index API, which returns the detailed metadata including transcripts, speakers, shots, and keyframes[7].
4.	Extract Key Images (Slides) and Transcript Segments: An Azure Function is then used to process the Video Indexer insights and prepare the content for the document. The function will parse the JSON to identify segment boundaries based on slide changes or speaker changes:
5.	Slide Change Detection: The insights include a list of shots or scenes with their start/end timestamps. Each shot represents a continuous sequence of frames without a camera/slide switch. Azure Video Indexer provides a representative keyframe image for each shot that “best represents the shot”[8] (for example, the first frame of a new slide). The function collects the timestamps where shots change – these are the slide transition points – and notes the associated keyframe images (each image is referenced by a thumbnailId or URL from Video Indexer).
6.	Speaker Change Detection: The function also scans the transcript for changes in the speakerId tag. Whenever the speaker identity changes from one segment to the next, that timestamp is marked as a break as well. (Video Indexer’s transcript entries include speaker IDs, enabling this detection[2].)
Combining these, the function builds an ordered timeline of section breakpoints – a sorted list of times where either a new slide begins or a new speaker starts talking. Each breakpoint will correspond to a new section in the final notes.
1.	Image Retrieval: For each section identified, an image from the video is obtained to represent that segment:
2.	If the section is triggered by a slide change (shot boundary), the precomputed keyframe from Video Indexer is used. The function calls the Video Indexer “Get Video Thumbnail” API (with the video ID and thumbnailId) to fetch the actual image file for that keyframe.
3.	If the section is due to a speaker transition without a visual slide change, the function can still capture a frame at that timestamp. This could be done by requesting a thumbnail at the specific time (Video Indexer thumbnails or using a lightweight Azure Function with FFmpeg on the blob video). In many cases, a speaker change in a video is accompanied by a camera change (e.g. switching to the new speaker’s view), which might already be a shot change. But to cover all scenarios, the design allows capturing an image at speaker transition time so each segment has an illustrative picture. All extracted images (JPEG/PNG) are saved to Blob Storage (e.g. in an “images” container) for inclusion in the document.
4.	Transcript Segmentation: The function groups the transcribed text according to those same breakpoints. It takes the transcript lines from the Video Indexer JSON (which are time-stamped and in sequence) and aggregates them into paragraphs for each section. For each segment between two breakpoints (or from a breakpoint to the next), it collates all transcript sentences that occurred in that time range. This yields a series of paragraphs, each corresponding to one image (slide or speaker segment). The text is cleaned up and formatted as needed (for example, removing timestamps, combining short phrases, and perhaps attributing speaker names if known).
5.	Document Generation (HTML/Markdown): The Azure Function then generates the final output content in the desired format. It can construct a Markdown string, where each section consists of an image followed by the transcript paragraph. For example, in Markdown:
 	![Slide 3](https://<storage>.blob.core.windows.net/images/slide3.png)  
*Transcript text for slide 3 or speaker segment...*
 	Each image reference uses the public URL or a short-lived SAS URL to the image stored in Blob Storage. The transcript text can be placed in a new paragraph beneath the image. (If using HTML, similarly an <img src="..."> tag with a <p> of text could be used.) This formatting ensures that when the file is imported into Notion, the images and text stay associated in the right order. The function ensures the Markdown/HTML adheres to Notion’s import guidelines (e.g. using standard Markdown that Notion can parse for images).
6.	Output Storage & Delivery: The composed Markdown (or HTML) content is then saved as a file (e.g. .md or .html) in Azure Blob Storage, typically in a dedicated container for outputs. Once saved, the Logic App can generate a secure download link. Finally, the Logic App returns an HTTP response to the user with a download URL (a blob SAS link) for the output file, or it could directly stream the file content back. The user can download this file and then import it into Notion. There is no direct integration with the Notion API – the user will just use Notion’s import feature to bring in the downloaded document.
(Optionally, the workflow can clean up temporary data: for example, an Azure Function could delete the original video blob and any intermediate images after a certain time or once the output is delivered, to minimize storage costs.)
Implementation Considerations
•	Serverless & Cost Efficiency: This design uses Azure Functions and Logic Apps, which are consumption-based (pay-per-use) services. There are no always-on servers – resources scale out automatically as needed and scale to zero when idle, aligning with a pay-as-you-go strategy[3]. This keeps costs low, since you only pay for the seconds of compute and memory when functions run and the minutes of video analyzed. Azure Video Indexer itself charges per-minute of video indexed, and can be linked to a free tier (2,400 free minutes for trial) or a paid Azure subscription as needed[9]. By using managed services (Video Indexer for analysis, Blob for storage), we avoid building a heavy custom pipeline, reducing development effort and cost.
•	Scalability: The architecture is inherently scalable. Each video request triggers an isolated workflow (Logic App instance or Durable Function) that can run in parallel with others. Azure Functions and Logic Apps will scale out to handle multiple videos simultaneously if demand increases. The most time-consuming part is the Video Indexer processing, which is handled by Azure’s cloud service that can process multiple videos concurrently (within service limits). Blob Storage can handle concurrent read/writes easily for storing files. This means the solution can scale from a few videos to many without redesign.
•	Trigger Points – Slides vs. Speakers: We use Azure Video Indexer’s AI to determine where to split the content. Slide changes are detected via Video Indexer’s shot detection, which finds abrupt or gradual scene transitions in the video[6]. In a recorded presentation, each new slide typically creates a new shot (visual change), so the keyframe image represents the new slide content. Speaker transitions are detected via speaker diarization – Video Indexer identifies and labels different speakers in the audio track[2]. Whenever the speaker label changes, we start a new section. By combining these, the notes will segment at all major transition points, matching the video’s flow. This ensures the images in the notes correspond either to a slide change or to a new speaker, as required.
•	Azure Integration: All components work together through Azure-managed triggers and APIs. The FAQ documentation confirms that Video Indexer can be integrated into custom workflows using Azure Logic Apps and Functions[4] – our architecture leverages this capability. The Logic App uses the Video Indexer connector or direct REST calls to start indexing and retrieve results. The connector abstracts away authentication (using an API key or Azure AD token for Video Indexer) and provides actions like Upload Video, Get Index, Get Thumbnail, etc., simplifying implementation. Azure Blob Storage integration is also straightforward: the function can use Azure SDKs to upload/download blobs, and Logic Apps has built-in actions to get or create blob content. By using these PaaS services, much of the heavy lifting (video processing, scaling, parallelism, etc.) is managed by Azure, allowing us to focus on the core logic of pairing images with transcripts.
•	Output Format for Notion: The choice of HTML or Markdown output is flexible. Markdown is often convenient for Notion import (Notion can import .md files with images), so the implementation can produce a Markdown file. We include the images via URLs; when Notion imports the file, it will fetch those image links and embed them into the Notion page. (Alternatively, the images could be uploaded separately, but using publicly accessible links in the Markdown keeps the process simple.) We ensure that the Markdown uses proper headings, bullet points, or other formatting only if needed (for example, the transcript could be in bullet points if that made sense, but here paragraphs suffice). The generated file is kept simple and clean for easy readability in Notion.
In summary, this architecture uses Azure Video Indexer’s AI capabilities to automatically extract the content needed – key frames and aligned transcript – and serverless Azure workflows to orchestrate everything. By leveraging Azure’s managed services, the solution achieves the required functionality (screenshots plus timed transcript) without any dedicated servers or manual intervention, and it remains cost-effective and scalable to handle videos on-demand[3]. The end result is a downloadable HTML/Markdown document that can be imported into Notion, containing a sequence of images with well-aligned lecture notes or meeting notes derived from the video. All requirements – support for public YouTube videos, slide/speaker-based segmentation, offline Notion import, and Azure-hosted serverless deployment – are satisfied by this design.
Sources: Azure Video Indexer documentation[1][8][2]; Azure serverless integration guides[4]; Azure Architecture best practices[3].
________________________________________
[1] Azure AI Video Indexer insights overview | Microsoft Learn
https://learn.microsoft.com/en-us/azure/azure-video-indexer/insights-overview
[2] [4] [9] Frequently asked questions about Azure AI Video Indexer - Azure | Microsoft Learn
https://learn.microsoft.com/en-us/azure/azure-video-indexer/faq
[3] [5] Automate Video Analysis by Using Azure Machine Learning and Azure AI Vision - Azure Architecture Center | Microsoft Learn
https://learn.microsoft.com/en-us/azure/architecture/ai-ml/architecture/analyze-video-computer-vision-machine-learning
[6] [8] Get scene, shot, and keyframe detection insights in Azure AI Video Indexer | Microsoft Learn
https://learn.microsoft.com/en-us/azure/azure-video-indexer/scene-shot-keyframe-detection-insight
[7] Retrieve video transcript from Azure Video Indexer using API - Microsoft Q&A
https://learn.microsoft.com/en-us/answers/questions/1662007/retrieve-video-transcript-from-azure-video-indexer
